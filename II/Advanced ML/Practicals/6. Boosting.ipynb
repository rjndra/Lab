{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting\n",
        "\n",
        "Boosting is an ensemble learning technique which builds a robust machine learning model by eliminating the weaknesses of weak learners. This is achieved through combining multiple weak learners which has high bias but less variance and turning them together into strong learners by minimizing errors. In boosting each training sample is assigned an equal weight, initially. Then, fitted with the model and for the samples with wrong prediction, weight for them is increased. Then the succeeding model tries to compensate for the weakness of its preeceding model. With each succeeeding model, the weak rules from preeceding model are utilized to form a strong model.\n",
        "\n",
        "For example, we have a task on hand to classify whether a given model is SPAM or NOT-SPAM. To classify emails as SPAM, say, we have to following rules learned by each learner:\n",
        "\n",
        "\n",
        "    Classifier 1: Emails that contains only links are SPAM.\n",
        "\n",
        "    Classifier 2: Emails That contains a word like 'million', '$', 'congratulations' etc. are SPAM.\n",
        "\n",
        "    Classifier 3: Emails from unknown senders are SPAM.\n",
        "\n",
        "\n",
        "\n",
        "Individually, these rules are not enough to categorize email as SPAN or NOT-SPAM. And model that learns a single rule is a weak model or weak learner. However, together all three models form a strong rule for SPAM classification.\n",
        "\n",
        "This is exactly what boosting does using different weak classifier in each iteration and forms a strong learner. To convert the weak learner to strong learner, we can combine the learning of each weak learner through\n",
        "\n",
        "a) Averaging\n",
        "\n",
        "b) Voting\n"
      ],
      "metadata": {
        "id": "12XCBnQLSQgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's now define what weak learners are?\n",
        "\n",
        "Weak learners are those which can predict better than random guessing. They are prone to overfitting and can't predict well on unseen data.\n",
        "\n",
        "## How training is done?\n",
        "\n",
        "Boosting creates an ensemble of such weak learner. For each data sample, algorithm assigns equal weights, initially. But assigns higher weights for those samples to which first weak learner predicts incorrectly. then a higher weight is assigned to those samples and the data is again fed to the second learner. Output from this second learner is again analyzed and for the samples with incorrect prediction, a higher weight is assigned. Then the new weighted inputs are again fed to the third learner and so on.\n",
        "\n",
        "Steps:\n",
        "1. First, equal weight is assigned to each data sample and first ML model is trained. The prediction from this model is analyzed.\n",
        "2. The boosting algorithm asses the output of first model and increases the weight of samples for which incorrect prediction was made by the first model. Then data wight new weight is again fed to the second model which makes prediction again. The output of second model is analyzed and same as above the new weight is assigned to the samples with incorrect predictions.\n",
        "3. The input is given to third model with new weight and prediction of it is assessed again\n",
        "4. This process continues until the error is below the expected level.\n",
        "\n",
        "The second model only focuses on the shortcommings of the first model. This is how the model improves their performance by correcting the mistakes the preceeding one did.\n",
        "\n",
        "## Matehmatical Model of Boosting\n",
        "Consider a classifer C that predicts among {1, -1} for any input X, then the error rate on each training sample is given as:\n",
        "\n",
        "\\begin{equation}\n",
        "err̅ = 1/N ∑_{i=1}^{N} I(y_i != C(x_i))\n",
        "\\end{equation}\n",
        "\n",
        "and the expected error rate on future predictions is $E_{XY}I(Y != C(X))$.\n",
        "\n",
        "The main purpose of boosting technique is to sequentially apply the weak classification algorithm to the weighted data such that a sequence of weak classifiers $C_m(x), m=1, 2,..., M$. The prediction from all of them are combined through a weighted average technique to produce a final prediction through strong classifer as:\n",
        "\\begin{equation}\n",
        "    C(x) = sign(∑_{m=1} ^{M} α_mC_m(x))\n",
        "\\end{equation}\n",
        "\n",
        "where, $α_i$ is computed by the boosting algorithm and is the weight contribution of each respective $C_m(x)$.\n",
        "\n",
        "Initially, all of the weights are set to $w_i = 1/N$ where there are N no. of training examples."
      ],
      "metadata": {
        "id": "akzcych3VsRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Boosting:\n",
        "There are three types of boosting:\n",
        "1. [Adaptive Boosting(AdaBoost)](https://colab.research.google.com/drive/1_cEMx22kpmLZ5vE63lxkq5AzMiaLdYyO#scrollTo=EGnqQmf2SK3t:~:text=1.%20adaptive%20boosting%20(adaboost))\n",
        "2. [Gradient Boosting]()\n",
        "3. [Extreme Gradient Boosting(XGBoost)]()\n"
      ],
      "metadata": {
        "id": "oC-cz5akYMms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Ada**ptive **Boost**ing (AdaBoost)\n",
        "This is the earliest realization of boosting technique which adapts to the weaknesses of previous model and self corrects in every iteration of boosting process. It is also called AdaBoost. The weak learner in AdaBoost are the decisiont rees with a single split which are called decision stumps. As mentioned above, AdaBoost also assigns the same weight to all the data samples of datasets. Then the weights are adjusted based on the prediction made by each model.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=12ruVBqh5LW-Lz-EJq76sw-rEsN4hJAOM\" alt=\"AdaBoost\" />\n",
        "\n",
        "**Notes**:\n",
        "* Earlier, decision trees were used as a learner. However, we can use any machine learning algorithm that accepts weight on training data set.\n",
        "* Earlier it was used for classification problem but we can use it for both classification and regression problem.\n",
        "* AdaBoost continiously builds a decision stumps that compensate on the weaknesses of previous decision stumps. This process continues until it has made the number of stumps we have asked for or until a expected fit is obtained."
      ],
      "metadata": {
        "id": "2K672DQUjFLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGnqQmf2SK3t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Import Adaptive Boosting library from Scikit Learn\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Import Decision Tree Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Import datasets\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Import training and test data splitter\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "# Import confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "breast_cancer_ds = load_breast_cancer()\n",
        "\n",
        "# Transform X dataframe\n",
        "X = pd.DataFrame(breast_cancer_ds['data'], columns = breast_cancer_ds['feature_names'])\n",
        "\n",
        "# Build a target dataframe\n",
        "y = pd.DataFrame(breast_cancer_ds['target'], columns = ['cancer_type'])\n",
        "\n",
        "# Check the shape of data\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiVaDF6kSOX4",
        "outputId": "ff9e02c0-ae68-4e22-d4d4-f0f70e45b216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((569, 30), (569, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Train test split of the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "# Create Decision Tree Classifier\n",
        "dtree_clf = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the classifier\n",
        "dtree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test samples\n",
        "y_pred = dtree_clf.predict(X_test)\n",
        "\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNNG5WC5dnP9",
        "outputId": "fbec0df3-e6d4-48f4-f566-ad911837d5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[32,  0],\n",
              "       [ 7, 75]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the Accuracy Score\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6mMcbpjhivf",
        "outputId": "d14c086b-c3a9-4aad-d215-0b54c56b19a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9385964912280702"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Adaboost Classifier\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    n_estimators=1000,\n",
        "    base_estimator=dtree_clf,\n",
        "    learning_rate = 1\n",
        "                                  )\n",
        "\n",
        "# Fit the AdaBoost Classifier\n",
        "adaboost_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test samples\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBypgKfgeJwV",
        "outputId": "2c47695d-7dd6-4edd-9aa4-6133c0debc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30,  2],\n",
              "       [ 9, 73]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSYTcWHDgTZq",
        "outputId": "08a2b686-6358-4224-8a2e-d2624570751f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9035087719298246"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Gradient Boosting\n"
      ],
      "metadata": {
        "id": "w2kTpv23jTMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Gradient Boosting Classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gboosting_clf = GradientBoostingClassifier(\n",
        "    n_estimators= 100,\n",
        "    learning_rate = 0.1,\n",
        "    max_depth = 1,\n",
        "    random_state = 7\n",
        ")\n",
        "gboosting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the classifier\n",
        "y_pred = gboosting_clf.predict(X_test)\n",
        "\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "j6H2sRLTiTYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab88ef4-5f6b-4889-fa12-d60a6b73f029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[31,  1],\n",
              "       [ 1, 81]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print accuracy\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryT5VNELxJOh",
        "outputId": "99ce3468-ceac-4c64-9cf4-6b4a02ebd1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9824561403508771"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}